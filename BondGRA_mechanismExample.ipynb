{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47769a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch\n",
    "\n",
    "def get_bdm(bonds, max_size):  # bond distance matrix\n",
    "    temp = torch.eye(max_size)\n",
    "    for i, bond1 in enumerate(bonds):\n",
    "        for j, bond2 in enumerate(bonds):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            if torch.unique(torch.cat([bond1, bond2])).size(0) < 4:  # at least on overlap\n",
    "                temp[i][j], temp[j][i] = 1, 1 # connect\n",
    "    return temp.unsqueeze(0).long()\n",
    "\n",
    "\n",
    "def pack_bond_feats(bonds_feats, pooled_bonds):\n",
    "    #print('pooled bonds are:\\n',pooled_bonds)\n",
    "    masks = [torch.ones(len(feats), dtype=torch.uint8) for feats in bonds_feats]\n",
    "    padded_feats = pad_sequence(bonds_feats, batch_first=True, padding_value= 0)\n",
    "    bdms = [get_bdm(bonds, padded_feats.size(1)) for bonds in pooled_bonds]\n",
    "    #print('bdms looks like this:\\n', bdms)\n",
    "    masks = pad_sequence(masks, batch_first=True, padding_value= 0)\n",
    "    return padded_feats, masks, torch.cat(bdms, dim = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99efb97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1, 3],\n",
       "         [3, 1],\n",
       "         [2, 4]]),\n",
       " tensor([[1, 3],\n",
       "         [4, 2]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_bonds = [torch.tensor([[1,3],[3,1],[2,4]]), torch.tensor([[1,3],[4,2]])]\n",
    "pooled_bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01924f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.rand(2,3,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "62c3a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "bonds_feats = [torch.tensor([[0.7613, 0.0477, 0.9314, 0.3316, 0.4118, 0.8059, 0.3685, 0.1201],\n",
    "                             [0.6702, 0.5265, 0.1083, 0.9956, 0.5293, 0.2533, 0.9514, 0.9963],\n",
    "                             [0.6594, 0.6021, 0.8603, 0.9002, 0.3984, 0.3284, 0.3642, 0.1200]]),\n",
    "               torch.tensor([[0.1940, 0.9269, 0.1595, 0.5202, 0.1896, 0.2814, 0.2581, 0.0624],\n",
    "                            [0.6663, 0.5718, 0.8896, 0.6519, 0.4934, 0.9678, 0.3778, 0.5658]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "47d81d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bond_feats, mask, bcms = pack_bond_feats(bonds_feats, pooled_bonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6421b9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7613, 0.0477, 0.9314, 0.3316, 0.4118, 0.8059, 0.3685, 0.1201],\n",
       "         [0.6702, 0.5265, 0.1083, 0.9956, 0.5293, 0.2533, 0.9514, 0.9963],\n",
       "         [0.6594, 0.6021, 0.8603, 0.9002, 0.3984, 0.3284, 0.3642, 0.1200]],\n",
       "\n",
       "        [[0.1940, 0.9269, 0.1595, 0.5202, 0.1896, 0.2814, 0.2581, 0.0624],\n",
       "         [0.6663, 0.5718, 0.8896, 0.6519, 0.4934, 0.9678, 0.3778, 0.5658],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6a323d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bf71ca95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 0],\n",
       "         [1, 1, 0],\n",
       "         [0, 0, 1]],\n",
       "\n",
       "        [[1, 0, 0],\n",
       "         [0, 1, 0],\n",
       "         [0, 0, 1]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e6001",
   "metadata": {},
   "source": [
    "Now I am ready for Attention!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0e5adee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Global_Reactivity_Attention(nn.Module):\n",
    "    def __init__(self, d_model, heads = 8, n_layers = 3, positional_number = 5, dropout = 0.1):\n",
    "        super(Global_Reactivity_Attention, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        att_stack = []\n",
    "        pff_stack = []\n",
    "        for _ in range(n_layers):\n",
    "            att_stack.append(MultiHeadAttention(heads, d_model, positional_number, dropout))\n",
    "            pff_stack.append(FeedForward(d_model, dropout=dropout))\n",
    "        self.att_stack = nn.ModuleList(att_stack)\n",
    "        self.pff_stack = nn.ModuleList(pff_stack)\n",
    "\n",
    "    def forward(self, x, rpm, mask = None):\n",
    "        att_scores = {}\n",
    "        for n in range(self.n_layers):\n",
    "            m, att_score = self.att_stack[n](x, rpm, mask)\n",
    "            x = x + self.pff_stack[n](x+m)\n",
    "            att_scores[n] = att_score\n",
    "        return x, att_scores\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "351911af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * torch.pow(x, 3)))) \n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*2),\n",
    "            GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model*2, d_model))\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        return self.net(x)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, positional_number = 5, dropout = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.p_k = positional_number\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        if self.p_k != 0:\n",
    "            self.relative_k = nn.Parameter(torch.randn(self.p_k, self.d_k))\n",
    "        self.q_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_linear = nn.Sequential(\n",
    "                            nn.Linear(d_model, d_model), \n",
    "                            nn.ReLU(), \n",
    "                            nn.Dropout(dropout),\n",
    "                            nn.Linear(d_model, d_model))\n",
    "        self.gating = nn.Linear(d_model, d_model)\n",
    "        self.to_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        nn.init.constant_(self.gating.weight, 0.)\n",
    "        nn.init.constant_(self.gating.bias, 1.)\n",
    "        \n",
    "    def one_hot_embedding(self, labels):\n",
    "        y = torch.eye(self.p_k)\n",
    "        return y[labels]\n",
    "                \n",
    "    def forward(self, x, gpm, mask=None):\n",
    "        bs, atom_size = x.size(0), x.size(1) #this line is done\n",
    "        x = self.layer_norm(x) \n",
    "        k = self.k_linear(x) #this line is done\n",
    "        q = self.q_linear(x) #this line is done\n",
    "        v = self.v_linear(x) #this line is done\n",
    "        k1 = k.view(bs, -1, self.h, self.d_k).transpose(1,2) #this line is done\n",
    "        q1 = q.view(bs, -1, self.h, self.d_k).transpose(1,2) #this line is done\n",
    "        v1 = v.view(bs, -1, self.h, self.d_k).transpose(1,2) #this line is done\n",
    "        attn1 = torch.matmul(q1, k1.permute(0, 1, 3, 2)) #this line is done\n",
    "        \n",
    "        if self.p_k == 0:\n",
    "            attn = attn1/math.sqrt(self.d_k)\n",
    "        else:\n",
    "            gpms = self.one_hot_embedding(gpm.unsqueeze(1).repeat(1, self.h, 1, 1)).to(x.device) #this line is done\n",
    "            attn2 = torch.matmul(q1, self.relative_k.transpose(0, 1)) #this line is done\n",
    "            attn2 = torch.matmul(gpms, attn2.unsqueeze(-1)).squeeze(-1) #this line is done\n",
    "            attn = (attn1 + attn2) /math.sqrt(self.d_k) #this line is done\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.bool() #this line is done\n",
    "            mask = mask.unsqueeze(1).repeat(1,mask.size(-1),1) #this line is done\n",
    "            mask = mask.unsqueeze(1).repeat(1,attn.size(1),1,1) #this line is done\n",
    "            attn[~mask] = float(-9e9) #this line is done\n",
    "        attn = torch.softmax(attn, dim=-1) #this line is done\n",
    "        attn = self.dropout1(attn) #this line is done\n",
    "        v1 = v.view(bs, -1, self.h, self.d_k).permute(0, 2, 1, 3) #this line is done\n",
    "        output = torch.matmul(attn, v1) #this line is done\n",
    "\n",
    "        output = output.transpose(1,2).contiguous().view(bs, -1, self.d_model).squeeze(-1)  #this line is done\n",
    "        output = self.to_out(output * self.gating(x).sigmoid()) # gate self attention\n",
    "        return self.dropout2(output), attn\n",
    "#         return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3ba830be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gra = Global_Reactivity_Attention(d_model=8, heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a7b8e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the instance variables\n",
    "#gra.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9ba31166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3, 3]) torch.Size([2, 2, 3, 4])\n",
      "torch.Size([2, 2, 3, 3]) torch.Size([2, 2, 3, 4])\n",
      "torch.Size([2, 2, 3, 3]) torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "bond_feat_new, att_scores = gra(x=bond_feats, rpm=bcms, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ddd93ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond_feat_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "021b9a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaa19ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f124e224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=8, bias=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 8\n",
    "q_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "q_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3b3986a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_q = q_linear(bond_feats)\n",
    "x_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ba92ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=8, bias=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same with k_linear\n",
    "d_model = 8\n",
    "k_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "k_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2ba962f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_k = k_linear(bond_feats)\n",
    "x_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7855373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v is a bit different; but the input and output dimension are the same\n",
    "v_linear = nn.Sequential(nn.Linear(d_model, d_model),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.1),\n",
    "                         nn.Linear(d_model, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da7935fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_v = v_linear(bond_feats)\n",
    "x_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a238f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the fun part of matrix manipulation and shape changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d59a919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 2\n",
    "h = 2\n",
    "d_k = 4\n",
    "k1 = x_k.view(bs, -1, h, d_k)\n",
    "k1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aa285c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b286c3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1 = k1.transpose(1,2)\n",
    "k1.shape #(B, H, N, dk); B= batch size; H= num heads; N = num bonds/atoms/seq len etc; dk = new feat dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc6f47ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 3, 4]), torch.Size([2, 2, 3, 4]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = x_q.view(bs, -1, h, d_k).transpose(1,2)\n",
    "v1 = x_v.view(bs, -1, h, d_k).transpose(1,2)\n",
    "q1.shape, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d7b9093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a395ad56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62e67981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1 = k1.permute(0,1,3,2)\n",
    "k1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24e674d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn1 = torch.matmul(q1, k1)\n",
    "attn1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8fda96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now understand the relative positional encoding\n",
    "#gpms = self.one_hot_embedding(gpm.unsqueeze(1).repeat(1, self.h, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39265ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 0],\n",
       "         [1, 1, 0],\n",
       "         [0, 0, 1]],\n",
       "\n",
       "        [[1, 0, 0],\n",
       "         [0, 1, 0],\n",
       "         [0, 0, 1]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc55aa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ef4af50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms = bcms.unsqueeze(1) #expected output shape: [2,1,3,3]\n",
    "bcms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b9809a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms = bcms.repeat(1, h, 1, 1) #expected output shape: [2,2,3,3]\n",
    "bcms.shape #now you see why this is done? It is the same shape as that of attention scores i.e. attn1.shape(2,2,3,3) QK^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82c24b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 1, 0],\n",
       "          [1, 1, 0],\n",
       "          [0, 0, 1]],\n",
       "\n",
       "         [[1, 1, 0],\n",
       "          [1, 1, 0],\n",
       "          [0, 0, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0],\n",
       "          [0, 1, 0],\n",
       "          [0, 0, 1]],\n",
       "\n",
       "         [[1, 0, 0],\n",
       "          [0, 1, 0],\n",
       "          [0, 0, 1]]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "385ab51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now what happens when I apply the one_hot_embedding function? Let's see.\n",
    "def one_hot_embedding(p_k, labels):\n",
    "    y = torch.eye(p_k)\n",
    "    return y[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "78118b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.eye(5)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31540da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3, 5])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpms = one_hot_embedding(5, bcms)\n",
    "gpms.shape #omg, I didn't see this coming; what is it doing?, okay now got it, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e392af57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0., 1., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.]],\n",
       "\n",
       "          [[0., 1., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.]],\n",
       "\n",
       "          [[1., 0., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 1., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.]],\n",
       "\n",
       "          [[0., 1., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.]],\n",
       "\n",
       "          [[1., 0., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.]],\n",
       "\n",
       "          [[1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.]],\n",
       "\n",
       "          [[1., 0., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.]],\n",
       "\n",
       "          [[1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.]],\n",
       "\n",
       "          [[1., 0., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]]]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ced7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58a06244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now next step;\n",
    "p_k = 5\n",
    "d_k = 4\n",
    "relative_k = nn.Parameter(torch.randn(p_k, d_k)) #it is just a random matrix of shape p_k, d_k; just that you have the requires_grad=True\n",
    "relative_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c22ec06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2876,  1.6704,  0.0232,  2.8696],\n",
       "        [ 0.2369,  0.8154, -1.8616, -0.2084],\n",
       "        [ 1.0022,  1.4363,  0.6675, -0.9453],\n",
       "        [-0.9335,  0.3653, -0.5655, -0.5385],\n",
       "        [ 2.0957, -0.9150, -0.3297, -0.7697]], requires_grad=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7bcb63ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05a69ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 5])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn2 = torch.matmul(q1, relative_k.T)\n",
    "attn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1a3faf7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 5, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn2 = attn2.unsqueeze(-1)\n",
    "attn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3da0e504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3, 5])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17265da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3, 1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn2 = torch.matmul(gpms, attn2)\n",
    "attn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9f4871bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn2 = attn2.squeeze(-1)\n",
    "attn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "850ff0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d3441f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = (attn1 + attn2) /math.sqrt(d_k)\n",
    "attn.shape #expected to be the same as that of attn1/attn2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5a326f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2112, -0.2452,  0.1531],\n",
       "          [ 0.0787, -0.0205,  0.1865],\n",
       "          [ 0.5586,  0.5176, -0.2876]],\n",
       "\n",
       "         [[ 0.0474,  0.0353,  0.2991],\n",
       "          [ 0.3091,  0.2454, -0.2282],\n",
       "          [-0.2213, -0.2191, -0.0989]]],\n",
       "\n",
       "\n",
       "        [[[-0.0184,  0.4468,  0.4570],\n",
       "          [ 0.3119, -0.2268,  0.3394],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.1037, -0.2947, -0.3047],\n",
       "          [-0.1731,  0.1510, -0.2842],\n",
       "          [ 0.0000,  0.0000,  0.0000]]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8339776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the masking part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4297fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = mask.bool()\n",
    "# mask = mask.unsqueeze(1).repeat(1,mask.size(-1),1)\n",
    "# mask = mask.unsqueeze(1).repeat(1,attn.size(1),1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1d6be996",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([[1, 1, 1],[1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c559c6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True],\n",
       "        [ True,  True, False]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "098a70f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.unsqueeze(1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4a34642a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.repeat(1, mask.size(-1), 1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e9eebd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 0],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 0]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4271912c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 0],\n",
       "          [1, 1, 0],\n",
       "          [1, 1, 0]]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.unsqueeze(1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "550d78cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3, 3])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "382ea736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.repeat(1, attn.size(1), 1, 1)\n",
    "mask.shape #now mask has the same shape as that of attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f60bd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn[~mask] = float(-9e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "618e386e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2f5e6916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = torch.softmax(attn, dim=-1)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2a1147ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = nn.Dropout(0.1)(attn)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "92c31fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704]],\n",
       "\n",
       "         [[0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704],\n",
       "          [0.0000, 0.0000, 0.3704]]],\n",
       "\n",
       "\n",
       "        [[[0.3704, 0.0000, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704]],\n",
       "\n",
       "         [[0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704]]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b532112c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 4])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the values modification\n",
    "#v.view(bs, -1, self.h, self.d_k).permute(0, 2, 1, 3); This I have done previously\n",
    "v1 = x_v.view(bs, -1, h, d_k)\n",
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "85976745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = v1.permute(0, 2, 1, 3)\n",
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b6b03f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "46ccf7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f9e3cc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the final equation application (attention score * values) i.e. the weighted sum\n",
    "output = torch.matmul(attn, v1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3382cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now concatenate all the heads\n",
    "#output = output.transpose(1,2).contiguous().view(bs, -1, self.d_model).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2a473476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 4])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.transpose(1,2)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3d226530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 4])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.contiguous() #ask why contiguos is necessary here\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1247575b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the combining\n",
    "output = output.view(bs, -1, d_model) #d_model is 8; remember?\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "566bdb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.squeeze(-1) #why this squeeze, I don't find the reason\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "69b963f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now gated self attention; extra\n",
    "#output = self.to_out(output * self.gating(x).sigmoid()) # gate self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3c58f78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond_feats.shape #this is my x, just that I have not passed through layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c6388290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=8, bias=True)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating = nn.Linear(d_model, d_model)\n",
    "gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "090a566c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_gate = gating(bond_feats)\n",
    "x_gate.shape #same expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b6ab30ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sigmoid = x_gate.sigmoid()\n",
    "x_sigmoid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "dd83fc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=8, bias=True)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_out = nn.Linear(d_model, d_model)\n",
    "to_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b927d5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first multiplication\n",
    "out = output * x_sigmoid\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "89745d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_final = to_out(out)\n",
    "output_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "73251169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final pass through another dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "561bc228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_final = nn.Dropout(0.1)(output_final)\n",
    "output_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOU ARE DONE!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdenv",
   "language": "python",
   "name": "rdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
