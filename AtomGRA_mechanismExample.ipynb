{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6d7b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47769a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_atom_feats(bg, atom_feats):\n",
    "    bg.ndata['h'] = atom_feats\n",
    "    gs = dgl.unbatch(bg)\n",
    "    edit_feats = [g.ndata['h'] for g in gs]\n",
    "    masks = [torch.ones(g.num_nodes(), dtype=torch.uint8) for g in gs]\n",
    "    padded_feats = pad_sequence(edit_feats, batch_first=True, padding_value= 0)\n",
    "    masks = pad_sequence(masks, batch_first=True, padding_value= 0)\n",
    "\n",
    "    return padded_feats, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87ba23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a batched graph of two small molecules ['COC', 'CC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6e170c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [torch.tensor([[-0.1575, -0.8111,  0.1657,  0.9680, -0.5555, 0.1657,  0.9680, -0.5555],\n",
    "                       [-0.7875, -0.4606,  1.0383,  0.2772,  0.8936, 0.1657,  0.9680, -0.5555],\n",
    "                       [-1.0585, -1.3510, -0.9072, -2.0390,  0.6899, 0.1657,  0.9680, -0.5555]]),\n",
    "         torch.tensor([[ 0.8600,  0.8323,  1.8069,  0.1786,  2.1482, 0.1657,  0.9680, -0.5555],\n",
    "                       [ 1.4895,  2.1187,  1.5305,  0.7236, -0.5476, 0.1657,  0.9680, -0.5555]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1f8e5318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1575, -0.8111,  0.1657,  0.9680, -0.5555,  0.1657,  0.9680, -0.5555],\n",
       "        [-0.7875, -0.4606,  1.0383,  0.2772,  0.8936,  0.1657,  0.9680, -0.5555],\n",
       "        [-1.0585, -1.3510, -0.9072, -2.0390,  0.6899,  0.1657,  0.9680, -0.5555]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats[0] #three atoms, three featurs set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "206dd7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8600,  0.8323,  1.8069,  0.1786,  2.1482,  0.1657,  0.9680, -0.5555],\n",
       "        [ 1.4895,  2.1187,  1.5305,  0.7236, -0.5476,  0.1657,  0.9680, -0.5555]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ba02d215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1.]), tensor([1., 1.])]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = [torch.ones(i.shape[0]) for i in feats]\n",
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c29b6ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_feats = pad_sequence(feats, batch_first=True, padding_value= 0)\n",
    "padded_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "68c5d5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1575, -0.8111,  0.1657,  0.9680, -0.5555,  0.1657,  0.9680,\n",
       "          -0.5555],\n",
       "         [-0.7875, -0.4606,  1.0383,  0.2772,  0.8936,  0.1657,  0.9680,\n",
       "          -0.5555],\n",
       "         [-1.0585, -1.3510, -0.9072, -2.0390,  0.6899,  0.1657,  0.9680,\n",
       "          -0.5555]],\n",
       "\n",
       "        [[ 0.8600,  0.8323,  1.8069,  0.1786,  2.1482,  0.1657,  0.9680,\n",
       "          -0.5555],\n",
       "         [ 1.4895,  2.1187,  1.5305,  0.7236, -0.5476,  0.1657,  0.9680,\n",
       "          -0.5555],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "293d41d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 0.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = pad_sequence(masks, batch_first=True, padding_value= 0)\n",
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09cc9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adm(mol, max_distance = 6):\n",
    "    mol_size = mol.GetNumAtoms()\n",
    "    distance_matrix = np.ones((mol_size, mol_size)) * max_distance + 1\n",
    "    dm = Chem.GetDistanceMatrix(mol)\n",
    "    dm[dm > 100] = -1 # remote (different molecule)\n",
    "    dm[dm > max_distance] = max_distance # remote (same molecule)\n",
    "    dm[dm == -1] = max_distance + 1\n",
    "    distance_matrix[:dm.shape[0],:dm.shape[1]] = dm\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ca08e250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 1., 2.],\n",
       "        [1., 0., 1.],\n",
       "        [2., 1., 0.]]),\n",
       " array([[0., 1.],\n",
       "        [1., 0.]])]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "\n",
    "adms = [get_adm(mol) for mol in [Chem.MolFromSmiles(k) for k in ['COC', 'CC']]]\n",
    "adms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dbe5ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_atom_distance_matrix(adm_list):\n",
    "    max_size = max([adm.shape[0] for adm in adm_list])\n",
    "    adm_list = [torch.tensor(np.pad(adm, (0, max_size - adm.shape[0]), 'maximum')).unsqueeze(0).long() for adm in adm_list]\n",
    "    return torch.cat(adm_list, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "318d01c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2],\n",
       "         [1, 0, 1],\n",
       "         [2, 1, 0]],\n",
       "\n",
       "        [[0, 1, 1],\n",
       "         [1, 0, 1],\n",
       "         [1, 1, 1]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adms = pad_atom_distance_matrix(adms)\n",
    "adms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180fc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e1e6001",
   "metadata": {},
   "source": [
    "Now I am ready for Attention!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0e5adee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Global_Reactivity_Attention(nn.Module):\n",
    "    def __init__(self, d_model, heads = 8, n_layers = 3, positional_number = 5, dropout = 0.1):\n",
    "        super(Global_Reactivity_Attention, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        att_stack = []\n",
    "        pff_stack = []\n",
    "        for _ in range(n_layers):\n",
    "            att_stack.append(MultiHeadAttention(heads, d_model, positional_number, dropout))\n",
    "            pff_stack.append(FeedForward(d_model, dropout=dropout))\n",
    "        self.att_stack = nn.ModuleList(att_stack)\n",
    "        self.pff_stack = nn.ModuleList(pff_stack)\n",
    "\n",
    "    def forward(self, x, rpm, mask = None):\n",
    "        att_scores = {}\n",
    "        for n in range(self.n_layers):\n",
    "            m, att_score = self.att_stack[n](x, rpm, mask)\n",
    "            x = x + self.pff_stack[n](x+m)\n",
    "            att_scores[n] = att_score\n",
    "        return x, att_scores\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "351911af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * torch.pow(x, 3)))) \n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*2),\n",
    "            GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model*2, d_model))\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        return self.net(x)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, positional_number = 5, dropout = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.p_k = positional_number\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        if self.p_k != 0:\n",
    "            self.relative_k = nn.Parameter(torch.randn(self.p_k, self.d_k))\n",
    "        self.q_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_linear = nn.Sequential(\n",
    "                            nn.Linear(d_model, d_model), \n",
    "                            nn.ReLU(), \n",
    "                            nn.Dropout(dropout),\n",
    "                            nn.Linear(d_model, d_model))\n",
    "        self.gating = nn.Linear(d_model, d_model)\n",
    "        self.to_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        nn.init.constant_(self.gating.weight, 0.)\n",
    "        nn.init.constant_(self.gating.bias, 1.)\n",
    "        \n",
    "    def one_hot_embedding(self, labels):\n",
    "        y = torch.eye(self.p_k)\n",
    "        return y[labels]\n",
    "                \n",
    "    def forward(self, x, gpm, mask=None):\n",
    "        bs, atom_size = x.size(0), x.size(1) #this line is done\n",
    "        x = self.layer_norm(x) \n",
    "        k = self.k_linear(x) #this line is done\n",
    "        q = self.q_linear(x) #this line is done\n",
    "        v = self.v_linear(x) #this line is done\n",
    "        k1 = k.view(bs, -1, self.h, self.d_k).transpose(1,2) #this line is done\n",
    "        q1 = q.view(bs, -1, self.h, self.d_k).transpose(1,2) #this line is done\n",
    "        v1 = v.view(bs, -1, self.h, self.d_k).transpose(1,2) #this line is done\n",
    "        attn1 = torch.matmul(q1, k1.permute(0, 1, 3, 2)) #this line is done\n",
    "        \n",
    "        if self.p_k == 0:\n",
    "            attn = attn1/math.sqrt(self.d_k)\n",
    "        else:\n",
    "            gpms = self.one_hot_embedding(gpm.unsqueeze(1).repeat(1, self.h, 1, 1)).to(x.device) #this line is done\n",
    "            attn2 = torch.matmul(q1, self.relative_k.transpose(0, 1)) #this line is done\n",
    "            attn2 = torch.matmul(gpms, attn2.unsqueeze(-1)).squeeze(-1) #this line is done\n",
    "            attn = (attn1 + attn2) /math.sqrt(self.d_k) #this line is done\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.bool() #this line is done\n",
    "            mask = mask.unsqueeze(1).repeat(1,mask.size(-1),1) #this line is done\n",
    "            mask = mask.unsqueeze(1).repeat(1,attn.size(1),1,1) #this line is done\n",
    "            attn[~mask] = float(-9e9) #this line is done\n",
    "        attn = torch.softmax(attn, dim=-1) #this line is done\n",
    "        attn = self.dropout1(attn) #this line is done\n",
    "        v1 = v.view(bs, -1, self.h, self.d_k).permute(0, 2, 1, 3) #this line is done\n",
    "        output = torch.matmul(attn, v1) #this line is done\n",
    "\n",
    "        output = output.transpose(1,2).contiguous().view(bs, -1, self.d_model).squeeze(-1)  #this line is done\n",
    "        output = self.to_out(output * self.gating(x).sigmoid()) # gate self attention\n",
    "        return self.dropout2(output), attn\n",
    "#         return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3ba830be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gra = Global_Reactivity_Attention(d_model=8, heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a7b8e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the instance variables\n",
    "#gra.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8a8b1abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9ba31166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 8]),)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_feat_new, att_scores = gra(x=padded_feats, rpm=adms, mask=masks)\n",
    "atom_feat_new.shape, #att_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "021b9a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eaa19ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "716c7dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=8, bias=False)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 8\n",
    "q_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "q_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cdf7b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_feats = padded_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fc485b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_q = q_linear(atom_feats)\n",
    "x_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "695ffbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=8, bias=False)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same with k_linear\n",
    "d_model = 8\n",
    "k_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "k_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "aef9ac69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_k = k_linear(atom_feats)\n",
    "x_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "91e5cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v is a bit different; but the input and output dimension are the same\n",
    "v_linear = nn.Sequential(nn.Linear(d_model, d_model),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.1),\n",
    "                         nn.Linear(d_model, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fc541d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_v = v_linear(atom_feats)\n",
    "x_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "28023441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the fun part of matrix manipulation and shape changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d0e124f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 4])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 2\n",
    "h = 2\n",
    "d_k = 4\n",
    "k1 = x_k.view(bs, -1, h, d_k)\n",
    "k1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6d57a3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4b582260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1 = k1.transpose(1,2)\n",
    "k1.shape #(B, H, N, dk); B= batch size; H= num heads; N = num bonds/atoms/seq len etc; dk = new feat dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "55fc65a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 3, 4]), torch.Size([2, 2, 3, 4]))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = x_q.view(bs, -1, h, d_k).transpose(1,2)\n",
    "v1 = x_v.view(bs, -1, h, d_k).transpose(1,2)\n",
    "q1.shape, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7c62a3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "abe6d51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9bacd49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4, 3])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1 = k1.permute(0,1,3,2)\n",
    "k1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4618774e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 3, 4]), torch.Size([2, 2, 4, 3]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.shape, k1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a16d57f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn1 = torch.matmul(q1, k1)\n",
    "attn1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ffd3a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now understand the relative positional encoding\n",
    "#gpms = self.one_hot_embedding(gpm.unsqueeze(1).repeat(1, self.h, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "58c1993e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2],\n",
       "         [1, 0, 1],\n",
       "         [2, 1, 0]],\n",
       "\n",
       "        [[0, 1, 1],\n",
       "         [1, 0, 1],\n",
       "         [1, 1, 1]]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms = adms\n",
    "bcms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5246387e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2],\n",
       "         [1, 0, 1],\n",
       "         [2, 1, 0]],\n",
       "\n",
       "        [[0, 1, 1],\n",
       "         [1, 0, 1],\n",
       "         [1, 1, 1]]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "53d86e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cbc76e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3, 3])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms = bcms.unsqueeze(1) #expected output shape: [2,1,3,3]\n",
    "bcms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1dc98f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms = bcms.repeat(1, h, 1, 1) #expected output shape: [2,2,3,3]\n",
    "bcms.shape #now you see why this is done? It is the same shape as that of attention scores i.e. attn1.shape(2,2,3,3) QK^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "240f594d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 2],\n",
       "          [1, 0, 1],\n",
       "          [2, 1, 0]],\n",
       "\n",
       "         [[0, 1, 2],\n",
       "          [1, 0, 1],\n",
       "          [2, 1, 0]]],\n",
       "\n",
       "\n",
       "        [[[0, 1, 1],\n",
       "          [1, 0, 1],\n",
       "          [1, 1, 1]],\n",
       "\n",
       "         [[0, 1, 1],\n",
       "          [1, 0, 1],\n",
       "          [1, 1, 1]]]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ad97da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now what happens when I apply the one_hot_embedding function? Let's see.\n",
    "def one_hot_embedding(p_k, labels):\n",
    "    y = torch.eye(p_k)\n",
    "    return y[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ca3f0775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.eye(5)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e4cb2193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3, 5])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpms = one_hot_embedding(5, bcms)\n",
    "gpms.shape #omg, I didn't see this coming; what is it doing?, okay now got it, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b3afc348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [0., 0., 1., 0., 0.]],\n",
       "\n",
       "          [[0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 1., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [0., 0., 1., 0., 0.]],\n",
       "\n",
       "          [[0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]],\n",
       "\n",
       "          [[0., 0., 1., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]],\n",
       "\n",
       "          [[0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]],\n",
       "\n",
       "          [[0., 1., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]],\n",
       "\n",
       "          [[0., 1., 0., 0., 0.],\n",
       "           [1., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]],\n",
       "\n",
       "          [[0., 1., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0.]]]]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241e4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4db87dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now next step;\n",
    "p_k = 5\n",
    "d_k = 4\n",
    "relative_k = nn.Parameter(torch.randn(p_k, d_k)) #it is just a random matrix of shape p_k, d_k; just that you have the requires_grad=True\n",
    "relative_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1744e72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3844, -0.3392, -0.2740,  0.5339],\n",
       "        [-0.5518,  1.3238, -1.3641,  0.8611],\n",
       "        [ 1.6464,  0.0230,  0.4377,  2.0879],\n",
       "        [-1.2769,  1.2342,  0.4180, -0.7119],\n",
       "        [ 1.5270, -0.3405, -0.4975,  1.3918]], requires_grad=True)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b1888e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "73edc1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 5])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn2 = torch.matmul(q1, relative_k.T)\n",
    "attn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d004af1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 5, 1])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn2 = attn2.unsqueeze(-1)\n",
    "attn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "86ead571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3, 5])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9b65b074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3, 1])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn2 = torch.matmul(gpms, attn2)\n",
    "attn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b3e79756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn2 = attn2.squeeze(-1)\n",
    "attn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "70222fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2336685f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = (attn1 + attn2) /math.sqrt(d_k)\n",
    "attn.shape #expected to be the same as that of attn1/attn2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6587d68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1167,  0.1943, -0.3456],\n",
       "          [ 0.1609, -0.0497,  0.0408],\n",
       "          [-0.1334,  0.0393,  0.0072]],\n",
       "\n",
       "         [[ 0.1182,  0.2237, -0.7956],\n",
       "          [ 0.1897,  0.3259, -0.0773],\n",
       "          [-0.3982,  0.5244, -0.0920]]],\n",
       "\n",
       "\n",
       "        [[[-0.0737, -0.2128, -0.2208],\n",
       "          [-0.6171, -0.6275, -0.7236],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.7220,  0.0400, -0.1517],\n",
       "          [ 0.5035,  0.0426,  0.2673],\n",
       "          [ 0.0000,  0.0000,  0.0000]]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0fcb7574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the masking part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "36eedc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = mask.bool()\n",
    "# mask = mask.unsqueeze(1).repeat(1,mask.size(-1),1)\n",
    "# mask = mask.unsqueeze(1).repeat(1,attn.size(1),1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "57f3702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([[1, 1, 1],[1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "02bd6a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True],\n",
       "        [ True,  True, False]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "714679fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.unsqueeze(1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "93c2125f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.repeat(1, mask.size(-1), 1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7fcf114e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 0],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 0]]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8683cd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 1, 1],\n",
       "          [1, 1, 1],\n",
       "          [1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 0],\n",
       "          [1, 1, 0],\n",
       "          [1, 1, 0]]]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.unsqueeze(1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d3293e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3, 3])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c0d93475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.repeat(1, attn.size(1), 1, 1)\n",
    "mask.shape #now mask has the same shape as that of attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "59c6ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn[~mask] = float(-9e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "33d96bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "eb3277d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = torch.softmax(attn, dim=-1)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4bbb278f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = nn.Dropout(0.1)(attn)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "cbd756b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3704, 0.0000, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.0000]],\n",
       "\n",
       "         [[0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.0000, 0.3704]]],\n",
       "\n",
       "\n",
       "        [[[0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704]],\n",
       "\n",
       "         [[0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704],\n",
       "          [0.3704, 0.3704, 0.3704]]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6e664607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 4])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the values modification\n",
    "#v.view(bs, -1, self.h, self.d_k).permute(0, 2, 1, 3); This I have done previously\n",
    "v1 = x_v.view(bs, -1, h, d_k)\n",
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e0f4cd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = v1.permute(0, 2, 1, 3)\n",
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "097f44cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "58d98bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b1b84968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the final equation application (attention score * values) i.e. the weighted sum\n",
    "output = torch.matmul(attn, v1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e40d9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now concatenate all the heads\n",
    "#output = output.transpose(1,2).contiguous().view(bs, -1, self.d_model).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b48b6576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 4])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.transpose(1,2)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "1a02a164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 4])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.contiguous() #ask why contiguos is necessary here\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4c0cc70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the combining\n",
    "output = output.view(bs, -1, d_model) #d_model is 8; remember?\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8c8408d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.squeeze(-1) #why this squeeze, I don't find the reason\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "11f2d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now gated self attention; extra\n",
    "#output = self.to_out(output * self.gating(x).sigmoid()) # gate self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "faaba661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_feats.shape #this is my x, just that I have not passed through layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3bc6b043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=8, bias=True)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating = nn.Linear(d_model, d_model)\n",
    "gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b36a8cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_gate = gating(atom_feats)\n",
    "x_gate.shape #same expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "99cf4404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sigmoid = x_gate.sigmoid()\n",
    "x_sigmoid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "4a9c6108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=8, bias=True)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_out = nn.Linear(d_model, d_model)\n",
    "to_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "aee06094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first multiplication\n",
    "out = output * x_sigmoid\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "71ec6a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_final = to_out(out)\n",
    "output_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "567212be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final pass through another dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f20d60b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_final = nn.Dropout(0.1)(output_final)\n",
    "output_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "97a460a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOU ARE DONE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e28504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2400aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdenv",
   "language": "python",
   "name": "rdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
